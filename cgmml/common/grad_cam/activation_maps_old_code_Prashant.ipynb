{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml\n",
    "from azureml.core import Workspace\n",
    "from azureml.core import Experiment\n",
    "from azureml.core.run import Run\n",
    "import os\n",
    "import glob2 as glob\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models, layers, callbacks, optimizers\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "#from preprocessing import preprocess_depthmap, preprocess_targets\n",
    "import argparse\n",
    "import cv2\n",
    "from tensorflow.keras.models import Model\n",
    "import matplotlib.pyplot as plt\n",
    "import math"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mount the workspace and data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azureml.core import Workspace, Dataset\n",
    "\n",
    "subscription_id = '9b82ecea-6780-4b85-8acf-d27d79028f07'\n",
    "resource_group = 'cgm-ml-prod'\n",
    "workspace_name = 'cgm-azureml-prod'\n",
    "\n",
    "workspace = Workspace(subscription_id, resource_group, workspace_name)\n",
    "\n",
    "dataset = Dataset.get_by_name(workspace, name='anon-depthmap-npy-test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with dataset.mount() as mount_context:\n",
    "       # list top level mounted files and folders in the dataset\n",
    "        print(os.listdir(mount_context.mount_point))\n",
    "print(mount_context.mount_point) \n",
    "#get the location of the mountpoint for your machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mount_context = dataset.mount()\n",
    "mount_context.start()  # this will mount the file streams\n",
    "print(mount_context.mount_point)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the image target size and define other hyperparameters\n",
    "# target_size = args.target_size[0].split(\"x\")\n",
    "image_target_width = 172\n",
    "image_target_height = 224\n",
    "# Get batch size and epochs\n",
    "batch_size = 2\n",
    "epochs = 50\n",
    "\n",
    "# Get the current run.\n",
    "# run = Run.get_context()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get dataset.\n",
    "dataset_path = \"../../../../../../npy\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the QR-code paths.\n",
    "print(\"Dataset path:\", dataset_path)\n",
    "print(glob.glob(os.path.join(dataset_path, \"*\"))) # Debug\n",
    "print(\"Getting QR-code paths...\")\n",
    "\n",
    "qrcode_paths = glob.glob(os.path.join(dataset_path, \"*\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split the data for training and validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Here only small dataset is used, bascially two scans so dividing the whole training and validation into 50%. Genral good practise \n",
    "## is 80-20%.\n",
    "split_index = int(len(qrcode_paths)*.5)\n",
    "qrcode_paths_training = qrcode_paths[:split_index]\n",
    "qrcode_paths_validate = qrcode_paths[split_index:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## This is dataset is used for visualising the activation maps. Due to small dataset , i am using training data as activation data.  \n",
    "activation_data = qrcode_paths_training[0]\n",
    "activation = [activation_data]\n",
    "print(qrcode_paths_training)\n",
    "print(activation_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_depthmap_files(paths):\n",
    "    pickle_paths = []\n",
    "    for path in paths:\n",
    "        pickle_paths.extend(glob.glob(os.path.join(path, \"**\", \"*.p\")))\n",
    "    return pickle_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "paths_training = get_depthmap_files(qrcode_paths_training)\n",
    "paths_validate = get_depthmap_files(qrcode_paths_validate)\n",
    "paths_activation = get_depthmap_files(activation_data)\n",
    "print(\"Using {} files for training.\".format(len(paths_training)))\n",
    "print(\"Using {} files for validation.\".format(len(paths_validate)))\n",
    "print(\"using {} files for activation.\".format(len(paths_activation)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for loading and processing depthmaps.\n",
    "def tf_load_pickle(path):\n",
    "\n",
    "    def py_load_pickle(path):\n",
    "        depthmap, targets = pickle.load(open(path.numpy(), \"rb\"))\n",
    "        depthmap = preprocess_depthmap(depthmap)\n",
    "        depthmap = tf.image.resize(depthmap, (image_target_height, image_target_width))\n",
    "        targets = preprocess_targets(targets, targets_indices)\n",
    "        return depthmap, targets\n",
    "\n",
    "    depthmap, targets = tf.py_function(py_load_pickle, [path], [tf.float32, tf.float32])\n",
    "    depthmap.set_shape((image_target_height, image_target_width, 1))\n",
    "    targets.set_shape((len(targets_indices,)))\n",
    "    return depthmap, targets\n",
    "\n",
    "def tf_flip(image):\n",
    "\n",
    "    image = tf.image.random_flip_left_right(image)\n",
    "    return image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Parameters for dataset generation.\n",
    "shuffle_buffer_size = 2\n",
    "subsample_size = 1\n",
    "channels = list(range(0, 3))\n",
    "targets_indices = [0] # 0 is height, 1 is weight.\n",
    "\n",
    "# Create dataset for training.\n",
    "paths = paths_training\n",
    "dataset = tf.data.Dataset.from_tensor_slices(paths)\n",
    "dataset = dataset.map(lambda path: tf_load_pickle(path))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.shuffle(shuffle_buffer_size)\n",
    "dataset = dataset.map(lambda image, label: (tf_flip(image), label))\n",
    "#dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "dataset_training = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for activation maps.\n",
    "paths = paths_activation\n",
    "dataset = tf.data.Dataset.from_tensor_slices(paths)\n",
    "dataset = dataset.map(lambda path: tf_load_pickle(path))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "dataset_check = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for activation maps.\n",
    "paths = paths_validate\n",
    "dataset = tf.data.Dataset.from_tensor_slices(paths)\n",
    "dataset = dataset.map(lambda path: tf_load_pickle(path))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "dataset_validate = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dataset for validation.\n",
    "# Note: No shuffle necessary.\n",
    "paths = paths_validate\n",
    "dataset = tf.data.Dataset.from_tensor_slices(paths)\n",
    "dataset = dataset.map(lambda path: tf_load_pickle(path))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "dataset_validate = dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d (Conv2D)              (None, 224, 172, 16)      160       \n",
      "_________________________________________________________________\n",
      "conv2d_1 (Conv2D)            (None, 224, 172, 16)      2320      \n",
      "_________________________________________________________________\n",
      "max_pooling2d (MaxPooling2D) (None, 112, 86, 16)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 112, 86, 32)       4640      \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 112, 86, 32)       9248      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 56, 43, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 56, 43, 64)        18496     \n",
      "_________________________________________________________________\n",
      "conv2d_5 (Conv2D)            (None, 56, 43, 64)        36928     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 28, 21, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 28, 21, 128)       73856     \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 28, 21, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 14, 10, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_8 (Conv2D)            (None, 14, 10, 256)       295168    \n",
      "_________________________________________________________________\n",
      "conv2d_9 (Conv2D)            (None, 14, 10, 256)       590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 5, 256)         0         \n",
      "_________________________________________________________________\n",
      "conv2d_10 (Conv2D)           (None, 7, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "conv2d_11 (Conv2D)           (None, 7, 5, 256)         590080    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_5 (MaxPooling2 (None, 3, 2, 256)         0         \n",
      "_________________________________________________________________\n",
      "flatten (Flatten)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dropout (Dropout)            (None, 1536)              0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 128)               196736    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 1)                 129       \n",
      "=================================================================\n",
      "Total params: 2,555,505\n",
      "Trainable params: 2,555,505\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# Instantiate model.\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\", activation=\"relu\", input_shape=(image_target_height, image_target_width, 1)))\n",
    "model.add(layers.Conv2D(filters=16, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(layers.Conv2D(filters=32, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(layers.Conv2D(filters=64, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(layers.Conv2D(filters=128, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(layers.Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(layers.Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(layers.Conv2D(filters=256, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "#model.add(layers.Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "#model.add(layers.Conv2D(filters=512, kernel_size=(3, 3), padding=\"same\", activation=\"relu\"))\n",
    "#model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Dense(128, activation=\"relu\"))\n",
    "model.add(layers.Dropout(0.25))\n",
    "model.add(layers.Dense(1, activation=\"linear\"))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'tensorflow.python.keras.engine.sequential.Sequential'>\n"
     ]
    }
   ],
   "source": [
    "print(type(model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## callbacks for training \n",
    "training_callbacks = []\n",
    "best_model_path = \"best_model.h5\"\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=best_model_path,\n",
    "    monitor=\"val_loss\", \n",
    "    save_best_only=True,\n",
    "    verbose=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradcam class to compute the heatmaps for the given layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GradCAM:\n",
    "    def __init__(self, model, layerName):\n",
    "        self.model = model\n",
    "        self.layerName = layerName\n",
    "    \n",
    "        self.gradModel = Model(inputs=[self.model.inputs], \n",
    "                                            outputs=[self.model.get_layer(self.layerName).output, model.output])\n",
    "    \n",
    "    def compute_heatmap(self, image, classIdx, eps=1e-8):\n",
    "    \n",
    "        with tf.GradientTape() as tape:\n",
    "            tape.watch(self.gradModel.get_layer(self.layerName).output)\n",
    "            inputs = tf.cast(image, tf.float32)\n",
    "            (convOutputs,predictions) = self.gradModel(inputs)\n",
    "            if len(predictions)==1:\n",
    "                loss = predictions[0]\n",
    "            else:\n",
    "                loss = predictions[:, classIdx]\n",
    "\n",
    "        grads = tape.gradient(loss, convOutputs)\n",
    "    \n",
    "        castConvOutputs = tf.cast(convOutputs > 0, \"float32\")\n",
    "        castGrads = tf.cast(grads > 0, \"float32\")\n",
    "        guidedGrads = castConvOutputs * castGrads * grads\n",
    "\n",
    "        convOutputs = convOutputs[0]\n",
    "        guidedGrads = guidedGrads[0]\n",
    "\n",
    "        weights = tf.reduce_mean(guidedGrads, axis=(0, 1))\n",
    "        cam = tf.reduce_sum(tf.multiply(weights, convOutputs), axis=-1)\n",
    "\n",
    "        (w, h) = (image.shape[2], image.shape[1])\n",
    "        heatmap = cv2.resize(cam.numpy(), (w, h))\n",
    "\n",
    "        numer = heatmap - np.min(heatmap)\n",
    "        denom = (heatmap.max() - heatmap.min()) + eps\n",
    "        heatmap = numer / denom\n",
    "        heatmap = (heatmap * 255).astype(\"float32\")\n",
    "        return heatmap\n",
    "        \n",
    "    \n",
    "#     def overlay_heatmap(self, heatmap, image, alpha=0.5, colormap=cv2.COLORMAP_HOT):\n",
    "#         heatmap = cv2.applyColorMap(heatmap, colormap)\n",
    "#         output = cv2.addWeighted(image, alpha, heatmap, 1 - alpha, 0)\n",
    "    \n",
    "#         return (heatmap, output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to create gridmap for the given set of images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_grid(image_dir):\n",
    "    from glob import glob\n",
    "    files = glob(image_dir+'/*.png')    \n",
    "    result_figsize_resolution = 80 # 1 = 100px\n",
    "\n",
    "    # images_list = os.listdir(images_dir)\n",
    "    images_count = 8\n",
    "    # Calculate the grid size:\n",
    "    grid_size = math.ceil(math.sqrt(images_count))\n",
    "\n",
    "    # Create plt plot:\n",
    "    fig, axes = plt.subplots(grid_size, grid_size, figsize=(result_figsize_resolution, result_figsize_resolution))\n",
    "    current_file_number = 0\n",
    "    samples = files[:images_count]\n",
    "    for image in samples:\n",
    "        x_position = current_file_number % grid_size\n",
    "        y_position = current_file_number // grid_size\n",
    "        plt_image = plt.imread(image)\n",
    "        axes[x_position, y_position].imshow(plt_image)\n",
    "    # print((current_file_number + 1), '/', images_count, ': ', image_filename)\n",
    "\n",
    "        current_file_number += 1\n",
    "\n",
    "    plt.subplots_adjust(left=0.0, right=1.0, bottom=0.0, top=1.0)\n",
    "    save_location = '{}/grid'.format(image_dir)\n",
    "    if not os.path.exists(save_location):            \n",
    "        os.makedirs(save_location)\n",
    "    plt.savefig('{}/resultgrid.png'.format(save_location))\n",
    "    plt.clf()\n",
    "    for file in files:\n",
    "        os.remove(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Keras callback for the gradcam visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GRADCamLogger(tf.keras.callbacks.Callback):\n",
    "    def __init__(self, activation_data, layer_name):\n",
    "        super(GRADCamLogger, self).__init__()\n",
    "        self.activation_data = activation_data\n",
    "        self.layer_name = layer_name\n",
    "        \n",
    "\n",
    "    def on_epoch_end(self,epoch,logs):\n",
    "        images = []\n",
    "        grad_cam = []\n",
    "      ## Initialize GRADCam Class\n",
    "        cam = GradCAM(self.model, self.layer_name)\n",
    "        count =0\n",
    "        foldername = 'out/epoch{}'.format(epoch)\n",
    "        if not os.path.exists(foldername):            \n",
    "            os.makedirs(foldername)           \n",
    "        for data in self.activation_data:\n",
    "            image = data[0]\n",
    "            image = np.expand_dims(image, 0)\n",
    "            pred = model.predict(image)\n",
    "            classIDx = np.argmax(pred[0])\n",
    "  \n",
    "        ## Compute Heatmap\n",
    "            heatmap = cam.compute_heatmap(image, classIDx)\n",
    "            image = image.reshape(image.shape[1:])\n",
    "            image = image*255\n",
    "            image = image.astype(np.uint8)\n",
    "\n",
    "        ## Overlay heatmap on original image\n",
    "            heatmap = cv2.resize(heatmap, (image.shape[1],image.shape[0]))\n",
    "            implot = plt.imshow(np.squeeze(image))            \n",
    "            plt.imshow(heatmap,alpha=.6,cmap='inferno')\n",
    "            plt.axis('off')\n",
    "            plt.savefig('out/epoch{}/out{}.png'.format(epoch,count), bbox_inches='tight', transparent=True,pad_inches=0)\n",
    "            plt.clf()\n",
    "            count+=1\n",
    "        make_grid(foldername)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##Adding Gradcam(Activation maps) callbacks into training  \n",
    "training_callbacks  =[]\n",
    "layer_name = 'conv2d_11'\n",
    "cam_callback = GRADCamLogger(dataset_check,layer_name)\n",
    "model.compile(\n",
    "    optimizer=\"nadam\",\n",
    "    loss=\"mse\",\n",
    "    metrics=[\"mae\"]\n",
    ")\n",
    "training_callbacks.append(checkpoint_callback)\n",
    "training_callbacks.append(cam_callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## training the model\n",
    "model.fit(\n",
    "    dataset_training.batch(batch_size),\n",
    "    validation_data=dataset_validate.batch(batch_size),\n",
    "    epochs=epochs,\n",
    "    callbacks=training_callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### convert the gridmaps pngs into gif for visualisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob2\n",
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.image as mpimg\n",
    "\n",
    "data = 'out'\n",
    "grid_data = glob2.glob('out/**/*.png')\n",
    "images =[]\n",
    "for filename in grid_data:\n",
    "    images.append(imageio.imread(filename))\n",
    "imageio.mimsave('movie.gif', images)\n",
    "img = mpimg.imread(\"movie.gif\")\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## stop the mont data\n",
    "mount_context.stop()"
   ]
  }
 ],
 "metadata": {
  "kernel_info": {
   "name": "python3-azureml"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  },
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
