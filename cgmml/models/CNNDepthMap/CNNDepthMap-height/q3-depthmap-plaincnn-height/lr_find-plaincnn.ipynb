{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import os\n",
    "import pickle\n",
    "import random\n",
    "import shutil\n",
    "\n",
    "import glob2 as glob\n",
    "import tensorflow as tf\n",
    "from azureml.core import Experiment, Workspace\n",
    "from azureml.core.run import Run\n",
    "from tensorflow.keras import callbacks\n",
    "\n",
    "sys.path.append(str(Path(os.getcwd()) / 'src'))\n",
    "\n",
    "from config import CONFIG\n",
    "from constants import REPO_DIR\n",
    "\n",
    "sys.path.append(str(REPO_DIR / 'cgmml/common'))\n",
    "\n",
    "from model_utils.model_plaincnn import create_cnn\n",
    "from model_utils.preprocessing import preprocess_depthmap, preprocess_targets\n",
    "from model_utils.lr_finder import LRFinder"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Make experiment reproducible\n",
    "tf.random.set_seed(CONFIG.SPLIT_SEED)\n",
    "random.seed(CONFIG.SPLIT_SEED)\n",
    "\n",
    "# Get the current run.\n",
    "run = Run.get_context()\n",
    "\n",
    "# Offline run. Download the sample dataset and run locally. Still push results to Azure.\n",
    "if(run.id.startswith(\"OfflineRun\")):\n",
    "    print(\"Running in offline mode...\")\n",
    "\n",
    "    # Access workspace.\n",
    "    print(\"Accessing workspace...\")\n",
    "    workspace = Workspace.from_config()\n",
    "    experiment = Experiment(workspace, \"training-junkyard\")\n",
    "    run = experiment.start_logging(outputs=None, snapshot_directory=None)\n",
    "\n",
    "    # Get dataset.\n",
    "    print(\"Accessing dataset...\")\n",
    "    dataset_name = \"anon-depthmap-mini\"\n",
    "    dataset_path = str(REPO_DIR / \"data\" / dataset_name)\n",
    "    if not os.path.exists(dataset_path):\n",
    "        dataset = workspace.datasets[dataset_name]\n",
    "        dataset.download(target_path=dataset_path, overwrite=False)\n",
    "\n",
    "# Online run. Use dataset provided by training notebook.\n",
    "else:\n",
    "    print(\"Running in online mode...\")\n",
    "    experiment = run.experiment\n",
    "    workspace = experiment.workspace\n",
    "    dataset_path = run.input_datasets[\"dataset\"]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Get the QR-code paths.\n",
    "dataset_path = os.path.join(dataset_path, \"scans\")\n",
    "print(\"Dataset path:\", dataset_path)\n",
    "#print(glob.glob(os.path.join(dataset_path, \"*\"))) # Debug\n",
    "print(\"Getting QR-code paths...\")\n",
    "qrcode_paths = glob.glob(os.path.join(dataset_path, \"*\"))\n",
    "print(\"qrcode_paths: \", len(qrcode_paths))\n",
    "assert len(qrcode_paths) != 0\n",
    "\n",
    "# Shuffle and split into train and validate.\n",
    "random.shuffle(qrcode_paths)\n",
    "split_index = int(len(qrcode_paths) * 0.8)\n",
    "qrcode_paths_training = qrcode_paths[:split_index]\n",
    "qrcode_paths_validate = qrcode_paths[split_index:]\n",
    "qrcode_paths_activation = random.choice(qrcode_paths_validate)\n",
    "qrcode_paths_activation = [qrcode_paths_activation]\n",
    "\n",
    "del qrcode_paths\n",
    "\n",
    "# Show split.\n",
    "print(\"Paths for training:\")\n",
    "print(\"\\t\" + \"\\n\\t\".join(qrcode_paths_training))\n",
    "print(\"Paths for validation:\")\n",
    "print(\"\\t\" + \"\\n\\t\".join(qrcode_paths_validate))\n",
    "print(\"Paths for activation:\")\n",
    "print(\"\\t\" + \"\\n\\t\".join(qrcode_paths_activation))\n",
    "\n",
    "print(len(qrcode_paths_training))\n",
    "print(len(qrcode_paths_validate))\n",
    "\n",
    "assert len(qrcode_paths_training) > 0 and len(qrcode_paths_validate) > 0\n",
    "\n",
    "\n",
    "def get_depthmap_files(paths):\n",
    "    pickle_paths = []\n",
    "    for path in paths:\n",
    "        pickle_paths.extend(glob.glob(os.path.join(path, \"**\", \"*.p\")))\n",
    "    return pickle_paths\n",
    "\n",
    "\n",
    "# Get the pointclouds.\n",
    "print(\"Getting depthmap paths...\")\n",
    "paths_training = get_depthmap_files(qrcode_paths_training)\n",
    "paths_validate = get_depthmap_files(qrcode_paths_validate)\n",
    "paths_activate = get_depthmap_files(qrcode_paths_activation)\n",
    "\n",
    "del qrcode_paths_training\n",
    "del qrcode_paths_validate\n",
    "del qrcode_paths_activation\n",
    "\n",
    "print(\"Using {} files for training.\".format(len(paths_training)))\n",
    "print(\"Using {} files for validation.\".format(len(paths_validate)))\n",
    "print(\"Using {} files for validation.\".format(len(paths_activate)))\n",
    "\n",
    "\n",
    "# Function for loading and processing depthmaps.\n",
    "def tf_load_pickle(path, max_value):\n",
    "    def py_load_pickle(path, max_value):\n",
    "        depthmap, targets = pickle.load(open(path.numpy(), \"rb\"))\n",
    "        depthmap = preprocess_depthmap(depthmap)\n",
    "        depthmap = depthmap / max_value\n",
    "        depthmap = tf.image.resize(depthmap, (CONFIG.IMAGE_TARGET_HEIGHT, CONFIG.IMAGE_TARGET_WIDTH))\n",
    "        targets = preprocess_targets(targets, CONFIG.TARGET_INDEXES)\n",
    "        return depthmap, targets\n",
    "\n",
    "    depthmap, targets = tf.py_function(py_load_pickle, [path, max_value], [tf.float32, tf.float32])\n",
    "    depthmap.set_shape((CONFIG.IMAGE_TARGET_HEIGHT, CONFIG.IMAGE_TARGET_WIDTH, 1))\n",
    "    targets.set_shape((len(CONFIG.TARGET_INDEXES,)))\n",
    "    return depthmap, targets"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "# Create dataset for training.\n",
    "paths = paths_training\n",
    "dataset = tf.data.Dataset.from_tensor_slices(paths)\n",
    "dataset = dataset.map(lambda path: tf_load_pickle(path, CONFIG.NORMALIZATION_VALUE))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "dataset = dataset.shuffle(CONFIG.SHUFFLE_BUFFER_SIZE)\n",
    "dataset_training = dataset\n",
    "del dataset\n",
    "\n",
    "# Create dataset for validation.\n",
    "# Note: No shuffle necessary.\n",
    "paths = paths_validate\n",
    "dataset = tf.data.Dataset.from_tensor_slices(paths)\n",
    "dataset = dataset.map(lambda path: tf_load_pickle(path, CONFIG.NORMALIZATION_VALUE))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "dataset_validation = dataset\n",
    "del dataset\n",
    "\n",
    "# Create dataset for activation\n",
    "paths = paths_activate\n",
    "dataset = tf.data.Dataset.from_tensor_slices(paths)\n",
    "dataset = dataset.map(lambda path: tf_load_pickle(path, CONFIG.NORMALIZATION_VALUE))\n",
    "dataset = dataset.cache()\n",
    "dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
    "dataset_activation = dataset\n",
    "del dataset\n",
    "\n",
    "# Note: Now the datasets are prepared.\n",
    "\n",
    "# Create the model.\n",
    "input_shape = (CONFIG.IMAGE_TARGET_HEIGHT, CONFIG.IMAGE_TARGET_WIDTH, 1)\n",
    "model = create_cnn(input_shape, dropout=True)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "model.compile(optimizer='adam', loss=\"mse\", metrics=[\"mae\"])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "# LR Find"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "lr_finder = LRFinder()\n",
    "_ = model.fit(dataset_training.batch(CONFIG.BATCH_SIZE), epochs=5, callbacks=[lr_finder], verbose=2)\n",
    "lr_finder.plot()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "len(dataset_training)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "file_extension": ".py",
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3.7.5 64-bit ('env_p_3': virtualenv)",
   "language": "python",
   "name": "python37564bitenvp3virtualenvba1e5b23cb4b48a69a71f222fe56e324"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  },
  "mimetype": "text/x-python",
  "name": "python",
  "npconvert_exporter": "python",
  "nteract": {
   "version": "nteract-front-end@1.0.0"
  },
  "pygments_lexer": "ipython3",
  "version": 3
 },
 "nbformat": 4,
 "nbformat_minor": 4
}